---
title: Policy Gradient Play with Networked Agents in Markov Potential Games
openreview: hm4AvehfBik
abstract: We introduce a distributed policy gradient play algorithm with networked
  agents playing Markov potential games. Agents have rewards at each stage of the
  game, that depend on the joint actions of agents given a common dynamic state. Agents
  implement parameterized and differentiable policies to take actions against each
  other. Markov potential assumes the existence of potential value functions. In a
  differentiable Markov potential game, partial gradients of a potential function
  are equal to the local gradients with respect to the individual parameters. In this
  work, agents receive information on other agents’ parameters via a communication
  network in addition to rewards. Agents then use stochastic gradients with respect
  to local estimates of joint policy parameters to update their policy parameters.
  We show that agents’ joint policy converges to a first-order stationary point of
  Markov potential value function with any type of function approximation, state and
  action spaces. Numerical experiments confirm the convergence result in the lake
  game, a Markov potential game.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: aydin23a
month: 0
tex_title: Policy Gradient Play with Networked Agents in Markov Potential Games
firstpage: 184
lastpage: 195
page: 184-195
order: 184
cycles: false
bibtex_author: Aydin, Sarper and Eksin, Ceyhun
author:
- given: Sarper
  family: Aydin
- given: Ceyhun
  family: Eksin
date: 2023-06-06
address:
container-title: Proceedings of The 5th Annual Learning for Dynamics and Control Conference
volume: '211'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 6
  - 6
pdf: https://proceedings.mlr.press/v211/aydin23a/aydin23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
