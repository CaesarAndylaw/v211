---
title: Modified Policy Iteration for Exponential Cost Risk Sensitive MDPs
openreview: crFypFQC0lO
abstract: Modified policy iteration (MPI) also known as optimistic policy iteration
  is at the core of many reinforcement learning algorithms. It works by combining
  elements of policy iteration and value iteration. The convergence of MPI has been
  well studied in the case of discounted and average-cost MDPs. In this work, we consider
  the exponential cost risk-sensitive MDP formulation, which is known to provide some
  robustness to model parameters. Although policy iteration and value iteration have
  been well studied in the context of risk sensitive MDPs, modified policy iteration
  is relatively unexplored. We provide the first proof that MPI also converges for
  the risk-sensitive problem in the case of finite state and action spaces. Since
  the exponential cost formulation deals with the multiplicative Bellman equation,
  our main contribution is a convergence proof which is quite different than existing
  results for discounted and risk-neutral average-cost problems.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: murthy23a
month: 0
tex_title: Modified Policy Iteration for Exponential Cost Risk Sensitive MDPs
firstpage: 395
lastpage: 406
page: 395-406
order: 395
cycles: false
bibtex_author: Murthy, Yashaswini and Moharrami, Mehrdad and Srikant, R.
author:
- given: Yashaswini
  family: Murthy
- given: Mehrdad
  family: Moharrami
- given: R.
  family: Srikant
date: 2023-06-06
address:
container-title: Proceedings of The 5th Annual Learning for Dynamics and Control Conference
volume: '211'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 6
  - 6
pdf: https://proceedings.mlr.press/v211/murthy23a/murthy23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
